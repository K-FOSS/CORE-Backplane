mysql-operator:
  # Default values for mysql-operator.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  replicaCount: 1

  image:
    repository: docker.io/bitpoke/mysql-operator
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""

  sidecar57:
    image:
      repository: docker.io/bitpoke/mysql-operator-sidecar-5.7
      # Overrides the image tag whose default is the chart appVersion.
      tag: ""

  sidecar80:
    image:
      repository: docker.io/bitpoke/mysql-operator-sidecar-8.0
      # Overrides the image tag whose default is the chart appVersion.
      tag: ""

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  extraEnv: []

  extraArgs: []

  rbac:
    create: true

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  podAnnotations: {}

  podSecurityContext:
    runAsNonRoot: true
    # 65532 is the UID for nonroot user from distroless image
    runAsUser: 65532
    runAsGroup: 65532
    fsGroup: 65532

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1

  podSecurityPolicy:
    enabled: false
    annotations:
      seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
      seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  # Insert a pre-stop lifecycle hook and trigger a failover. NOTE: Use this when your cluster network
  # policy allows to connect across namespaces and the mysql node is able to connecto to operator pod
  gracefulShutdown:
    enabled: true

  # in which namespace to watch for resource, leave empty to watch in all namespaces
  watchNamespace:

  # Install a ServiceMonitor for monitoring the operator
  serviceMonitor:
    # enabled should be set to true to enable prometheus-operator discovery of this service
    enabled: false
    # the Service port.name where prometheus metrics are exposed
    servicePortName: prometheus
    # the Service port.port where metrics are exposed
    servicePort: 9125
    # interval is the interval at which metrics should be scraped
    # interval: 30s
    # scrapeTimeout is the timeout after which the scrape is ended
    # scrapeTimeout: 10s
    # additionalLabels is the set of additional labels to add to the ServiceMonitor
    additionalLabels: {}
    jobLabel: ""
    targetLabels: []
    podTargetLabels: []
    metricRelabelings: []

  # The operator will install a ServiceMonitor if you have prometheus-operator installed.
  mysqlClustersGlobalServiceMonitor:
    enabled: false
    ## Additional labels for the serviceMonitor. Useful if you have multiple prometheus operators running to select only specific ServiceMonitors
    # additionalLabels:
    #   prometheus: prom-internal
    interval: 10s
    scrapeTimeout: 3s
    # jobLabel:
    # targetLabels:
    # podTargetLabels:
    namespaceSelector:
      any: true
    selector:
      matchLabels:
        app.kubernetes.io/managed-by: mysql.presslabs.org
        app.kubernetes.io/name: mysql

  orchestrator:
    image:
      repository: docker.io/bitpoke/mysql-operator-orchestrator
      pullPolicy: IfNotPresent
      # Overrides the image tag whose default is the chart appVersion.
      tag: ""

    securityContext: {}
      # capabilities:
      #   drop:
      #   - ALL
      # readOnlyRootFilesystem: true
      # runAsNonRoot: true
      # runAsUser: 1000

    # secretName:  # specify an existing secret to use for orchestrator topology credentials

    # if a secret is not specified one will be created for orchestrator user and password used to manage MySQL clusters
    topologyUser: orchestrator
    topologyPassword:  # this is empty and will be random generated if not specified

    resources: {}
      # For example
      # limits:
      #  cpu: 100m
      #  memory: 128Mi
      # requests:
      #  cpu: 100m
      #  memory: 128Mi

    service:
      type: ClusterIP
      port: 80
      # nodePort: 3000

    ingress:
      enabled: false
      className: ""
      annotations: {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
      hosts:
        - host: chart-example.local
          paths:
            - path: /
              pathType: ImplementationSpecific
      tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local


    persistence:
      enabled: true
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"
      # annotations: {}
      # selector:
      #  matchLabels: {}
      accessMode: "ReadWriteOnce"
      size: 1Gi
      # inject an init container which properly sets the ownership for the orchestrator's data volume
      # this is needed when the PV provisioner does not properly sets permissions for fsGroup
      # when enabling this, you MUST change the securityContext.runAsNonRoot to false
      fsGroupWorkaroundEnabled: false

    # key value map of orchestrator conf directives.
    # see: https://github.com/github/orchestrator/blob/master/conf/orchestrator-sample.conf.json
    # the following keys are manages and thus cannot be overwritten:
    #   - ListenAddress :3000
    #   - MySQLTopologyCredentialsConfigFile /orchestrator/conf/orc-topology.cnf
    #   - BackendDB sqlite
    #   - SQLite3DataFile /var/lib/orchestrator/orc.db
    #   - RaftEnabled true
    #   - RaftDataDir /var/lib/orchestrator
    #   - RaftBind $HOSTNAME
    #   - RaftNodes The statefullset members
    config:
      Debug: false
      # the operator is handling the registries, do not auto discover
      DiscoverByShowSlaveHosts: false
      # forget missing instances automatically
      UnseenInstanceForgetHours: 1

      InstancePollSeconds: 5
      HostnameResolveMethod: "none"
      MySQLHostnameResolveMethod: "@@report_host"
      RemoveTextFromHostnameDisplay: ":3306"
      DetectClusterAliasQuery: "SELECT CONCAT(SUBSTRING(@@hostname, 1, LENGTH(@@hostname) - 1 - LENGTH(SUBSTRING_INDEX(@@hostname,'-',-2))),'.',SUBSTRING_INDEX(@@report_host,'.',-1))"
      DetectInstanceAliasQuery: "SELECT @@hostname"
      SlaveLagQuery: "SELECT TIMESTAMPDIFF(SECOND,ts,UTC_TIMESTAMP()) as drift FROM sys_operator.heartbeat ORDER BY drift ASC LIMIT 1"

      # Automated recovery (this is opt-in, so we need to set these)
      # Prevent recovery flip-flop, by disabling auto-recovery for 5 minutes per
      # cluster
      RecoveryPeriodBlockSeconds: 300
      # Do not ignore any host for auto-recovery
      RecoveryIgnoreHostnameFilters: []
      # Recover both, masters and intermediate masters
      RecoverMasterClusterFilters: ['.*']
      RecoverIntermediateMasterClusterFilters: ['.*']
      # `reset slave all` and `set read_only=0` on promoted master
      ApplyMySQLPromotionAfterMasterFailover: true
      # https://github.com/github/orchestrator/blob/master/docs/configuration-recovery.md#promotion-actions
      # Safety! do not disable unless you know what you are doing
      FailMasterPromotionIfSQLThreadNotUpToDate: true
      DetachLostReplicasAfterMasterFailover: true
      # set downtime on the failed master
      MasterFailoverLostInstancesDowntimeMinutes: 10

      # orchestrator hooks called in the following order
      # for more information about template: https://github.com/github/orchestrator/blob/master/go/logic/topology_recovery.go#L256
      ProcessesShellCommand: "sh"

      OnFailureDetectionProcesses:
        - "/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcFailureDetection' 'Failure: {failureType}, failed host: {failedHost}, lost replcas: {lostReplicas}' || true"
        - "/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true"

      # PreGracefulTakeoverProcesses:
      PreFailoverProcesses:
        # as backup in case the first request fails
        - "/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true"
      # PostFailoverProcesses:
      #   - "/usr/local/bin/orchestrator-helper event '{failureClusterAlias}' 'Orc{command}' 'Failure type: {failureType}, failed hosts: {failedHost}, slaves: {countSlaves}' || true"

      PostUnsuccessfulFailoverProcesses:
        - "/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcPostUnsuccessfulFailover' 'Failure: {failureType}, failed host: {failedHost} with {countSlaves} slaves' || true"

      PostMasterFailoverProcesses:
        - "/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostMasterFailover' 'Failure type: {failureType}, new master: {successorHost}, slaves: {slaveHosts}' || true"

      PostIntermediateMasterFailoverProcesses:
        - "/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostIntermediateMasterFailover' 'Failure type: {failureType}, failed hosts: {failedHost}, slaves: {countSlaves}' || true"

      # PostGracefulTakeoverProcesses:

mariadb-operator:
  nameOverride: ""
  fullnameOverride: ""

  image:
    repository: ghcr.io/mariadb-operator/mariadb-operator
    pullPolicy: IfNotPresent
    # -- Image tag to use. By default the chart appVersion is used
    tag: ""

  # -- Controller log level
  logLevel: INFO

  # -- Cluster DNS name
  clusterName: cluster.local

  ha:
    # -- Enable high availability
    enabled: false
    # -- Number of replicas
    replicas: 3
    # -- Lease resource name to be used for leader election
    leaseId: mariadb.mmontes.io

  metrics:
    # -- Enable prometheus metrics. Prometheus must be installed in the cluster
    enabled: false
    serviceMonitor:
      # -- Enable controller ServiceMonitor
      enabled: true
      # -- Labels to be added to the controller ServiceMonitor
      additionalLabels: {}
      # release: kube-prometheus-stack
      # --  Interval to scrape metrics
      interval: 30s
      # -- Timeout if metrics can't be retrieved in given time interval
      scrapeTimeout: 25s

  webhook:
    # -- Enable webhooks. Cert-manager must be installed in the cluster
    enabled: true
    # -- Annotations for webhook configurations.
    annotations: {}
    image:
      repository: ghcr.io/mariadb-operator/mariadb-operator
      pullPolicy: IfNotPresent
      # -- Image tag to use. By default the chart appVersion is used
      tag: ""
    certificate:
      # -- Use cert-manager to issue and rotate the certificate. If set to false, a default certificate will be used.
      certManager: true
      # -- Default certificate generated when the chart is installed or upgraded.
      default:
        # -- Certificate authority expiration in days.
        caExpirationDays: 365
        # -- Certificate expiration in days.
        certExpirationDays: 365
        # -- Annotations for certificate Secret.
        annotations: {}
      # -- Path where the certificate will be mounted.
      path: /tmp/k8s-webhook-server/serving-certs
    # -- Port to be used by the webhook server
    port: 10250
    # -- Expose the webhook server in the host network
    hostNetwork: false
    serviceMonitor:
      # -- Enable webhook ServiceMonitor. Metrics must be enabled
      enabled: true
      # -- Labels to be added to the webhook ServiceMonitor
      additionalLabels: {}
      # release: kube-prometheus-stack
      # --  Interval to scrape metrics
      interval: 30s
      # -- Timeout if metrics can't be retrieved in given time interval
      scrapeTimeout: 25s
    # -- Annotations to add to webhook Pod
    podAnnotations: {}
    # -- Security context to add to webhook Pod
    podSecurityContext: {}
    # -- Security context to add to webhook container
    securityContext: {}
    # -- Resources to add to webhook container
    resources: {}
    # requests:
    #   cpu: 10m
    #   memory: 32Mi
    # -- Node selectors to add to controller Pod
    nodeSelector: {}
    # -- Tolerations to add to controller Pod
    tolerations: []
    # -- Affinity to add to controller Pod
    affinity: {}

  # -- Annotations to add to controller Pod
  podAnnotations: {}

  # -- Security context to add to controller Pod
  podSecurityContext: {}

  # -- Security context to add to controller container
  securityContext: {}

  resources: {}
  # -- Resources to add to controller container
  # requests:
  #   cpu: 10m
  #   memory: 32Mi

  # -- Node selectors to add to controller Pod
  nodeSelector: {}

  # -- Tolerations to add to controller Pod
  tolerations: []

  # -- Affinity to add to controller Pod
  affinity: {}


mysql:
  # Default values for mysql.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  image:
    registry: 
    repository: mysql
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    #tag: 8.0.33
    tag: 5.7
    xtraBackupRepository: perconalab/percona-xtrabackup

  ## MySQL Cluster parameters
  cluster:
    ## CLUSTER_ID
    clusterId: "1"
    ## CLUSTER_START_INDEX
    clusterStartIndex: "1"
    ## @param cluster.replicaSetCount
    replicaSetCount: 3
    ## MYSQL_TEMPLATE_CONFIG
    templateConfig:
    ## MYSQL_CUSTOM_CONFIG
    customConfig:
    ## MYSQL_DYNAMIC_CONFIG
    dynamicConfig:
    ## KB_EMBEDDED_WESQL
    kbWeSQLImage: "1"

  ## MySQL Authentication parameters
  auth:
    ## MYSQL_ROOT_HOST
    rootHost: "%"
    ## @param auth.createDatabase Whether to create the .Values.auth.database or not
    ##
    createDatabase: true
    ## @param auth.database Name for a custom database to create
    ## MYSQL_DATABASE
    database: "mydb"
    ## @param auth.username Name for a custom user to create
    ## MYSQL_USER
    username: "u1"
    ## MYSQL_PASSWORD
    password: "u1"
    ## @param auth.replicationUser MySQL replication user
    ##
    replicationUser: "replicator"
    ## @param auth.replicationPassword MySQL replication user password. Ignored if existing secret is provided
    ##
    replicationPassword: ""


  configTemplate:
    ## @param config template name
    ## name: mysql-3node-tpl-8.0

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  clusterVersionOverride: ""

  logConfigs:
    error: /var/lib/mysql/log/mysqld-error.log
    slow: /var/lib/mysql/log/mysqld-slowquery.log
    # general: /var/lib/mysql/log/mysqld.log

  roleProbe:
    failureThreshold: 2
    periodSeconds: 1
    timeoutSeconds: 1

  metrics:
    image:
      repository: apecloud/agamotto
      tag: 0.1.2-beta.1
      pullPolicy: IfNotPresent

    service:
      port: 9104

  dataMountPath: /var/lib/mysql

mysql-operator:
  # Default values for mysql-operator.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  replicaCount: 1

  image:
    repository: docker.io/bitpoke/mysql-operator
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""

  sidecar57:
    image:
      repository: docker.io/bitpoke/mysql-operator-sidecar-5.7
      # Overrides the image tag whose default is the chart appVersion.
      tag: ""

  sidecar80:
    image:
      repository: docker.io/bitpoke/mysql-operator-sidecar-8.0
      # Overrides the image tag whose default is the chart appVersion.
      tag: ""

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  extraEnv: []

  extraArgs: []

  rbac:
    create: true

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  podAnnotations: {}

  podSecurityContext:
    runAsNonRoot: true
    # 65532 is the UID for nonroot user from distroless image
    runAsUser: 65532
    runAsGroup: 65532
    fsGroup: 65532

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1

  podSecurityPolicy:
    enabled: false
    annotations:
      seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
      seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  # Insert a pre-stop lifecycle hook and trigger a failover. NOTE: Use this when your cluster network
  # policy allows to connect across namespaces and the mysql node is able to connecto to operator pod
  gracefulShutdown:
    enabled: true

  # in which namespace to watch for resource, leave empty to watch in all namespaces
  watchNamespace:

  # Install a ServiceMonitor for monitoring the operator
  serviceMonitor:
    # enabled should be set to true to enable prometheus-operator discovery of this service
    enabled: false
    # the Service port.name where prometheus metrics are exposed
    servicePortName: prometheus
    # the Service port.port where metrics are exposed
    servicePort: 9125
    # interval is the interval at which metrics should be scraped
    # interval: 30s
    # scrapeTimeout is the timeout after which the scrape is ended
    # scrapeTimeout: 10s
    # additionalLabels is the set of additional labels to add to the ServiceMonitor
    additionalLabels: {}
    jobLabel: ""
    targetLabels: []
    podTargetLabels: []
    metricRelabelings: []

  # The operator will install a ServiceMonitor if you have prometheus-operator installed.
  mysqlClustersGlobalServiceMonitor:
    enabled: false
    ## Additional labels for the serviceMonitor. Useful if you have multiple prometheus operators running to select only specific ServiceMonitors
    # additionalLabels:
    #   prometheus: prom-internal
    interval: 10s
    scrapeTimeout: 3s
    # jobLabel:
    # targetLabels:
    # podTargetLabels:
    namespaceSelector:
      any: true
    selector:
      matchLabels:
        app.kubernetes.io/managed-by: mysql.presslabs.org
        app.kubernetes.io/name: mysql

  orchestrator:
    image:
      repository: docker.io/bitpoke/mysql-operator-orchestrator
      pullPolicy: IfNotPresent
      # Overrides the image tag whose default is the chart appVersion.
      tag: ""

    securityContext: {}
      # capabilities:
      #   drop:
      #   - ALL
      # readOnlyRootFilesystem: true
      # runAsNonRoot: true
      # runAsUser: 1000

    # secretName:  # specify an existing secret to use for orchestrator topology credentials

    # if a secret is not specified one will be created for orchestrator user and password used to manage MySQL clusters
    topologyUser: orchestrator
    topologyPassword:  # this is empty and will be random generated if not specified

    resources: {}
      # For example
      # limits:
      #  cpu: 100m
      #  memory: 128Mi
      # requests:
      #  cpu: 100m
      #  memory: 128Mi

    service:
      type: ClusterIP
      port: 80
      # nodePort: 3000

    ingress:
      enabled: false
      className: ""
      annotations: {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
      hosts:
        - host: chart-example.local
          paths:
            - path: /
              pathType: ImplementationSpecific
      tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local


    persistence:
      enabled: true
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"
      # annotations: {}
      # selector:
      #  matchLabels: {}
      accessMode: "ReadWriteOnce"
      size: 1Gi
      # inject an init container which properly sets the ownership for the orchestrator's data volume
      # this is needed when the PV provisioner does not properly sets permissions for fsGroup
      # when enabling this, you MUST change the securityContext.runAsNonRoot to false
      fsGroupWorkaroundEnabled: false

    # key value map of orchestrator conf directives.
    # see: https://github.com/github/orchestrator/blob/master/conf/orchestrator-sample.conf.json
    # the following keys are manages and thus cannot be overwritten:
    #   - ListenAddress :3000
    #   - MySQLTopologyCredentialsConfigFile /orchestrator/conf/orc-topology.cnf
    #   - BackendDB sqlite
    #   - SQLite3DataFile /var/lib/orchestrator/orc.db
    #   - RaftEnabled true
    #   - RaftDataDir /var/lib/orchestrator
    #   - RaftBind $HOSTNAME
    #   - RaftNodes The statefullset members
    config:
      Debug: false
      # the operator is handling the registries, do not auto discover
      DiscoverByShowSlaveHosts: false
      # forget missing instances automatically
      UnseenInstanceForgetHours: 1

      InstancePollSeconds: 5
      HostnameResolveMethod: "none"
      MySQLHostnameResolveMethod: "@@report_host"
      RemoveTextFromHostnameDisplay: ":3306"
      DetectClusterAliasQuery: "SELECT CONCAT(SUBSTRING(@@hostname, 1, LENGTH(@@hostname) - 1 - LENGTH(SUBSTRING_INDEX(@@hostname,'-',-2))),'.',SUBSTRING_INDEX(@@report_host,'.',-1))"
      DetectInstanceAliasQuery: "SELECT @@hostname"
      SlaveLagQuery: "SELECT TIMESTAMPDIFF(SECOND,ts,UTC_TIMESTAMP()) as drift FROM sys_operator.heartbeat ORDER BY drift ASC LIMIT 1"

      # Automated recovery (this is opt-in, so we need to set these)
      # Prevent recovery flip-flop, by disabling auto-recovery for 5 minutes per
      # cluster
      RecoveryPeriodBlockSeconds: 300
      # Do not ignore any host for auto-recovery
      RecoveryIgnoreHostnameFilters: []
      # Recover both, masters and intermediate masters
      RecoverMasterClusterFilters: ['.*']
      RecoverIntermediateMasterClusterFilters: ['.*']
      # `reset slave all` and `set read_only=0` on promoted master
      ApplyMySQLPromotionAfterMasterFailover: true
      # https://github.com/github/orchestrator/blob/master/docs/configuration-recovery.md#promotion-actions
      # Safety! do not disable unless you know what you are doing
      FailMasterPromotionIfSQLThreadNotUpToDate: true
      DetachLostReplicasAfterMasterFailover: true
      # set downtime on the failed master
      MasterFailoverLostInstancesDowntimeMinutes: 10

      # orchestrator hooks called in the following order
      # for more information about template: https://github.com/github/orchestrator/blob/master/go/logic/topology_recovery.go#L256
      ProcessesShellCommand: "sh"

      OnFailureDetectionProcesses:
        - "/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcFailureDetection' 'Failure: {failureType}, failed host: {failedHost}, lost replcas: {lostReplicas}' || true"
        - "/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true"

      # PreGracefulTakeoverProcesses:
      PreFailoverProcesses:
        # as backup in case the first request fails
        - "/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true"
      # PostFailoverProcesses:
      #   - "/usr/local/bin/orchestrator-helper event '{failureClusterAlias}' 'Orc{command}' 'Failure type: {failureType}, failed hosts: {failedHost}, slaves: {countSlaves}' || true"

      PostUnsuccessfulFailoverProcesses:
        - "/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcPostUnsuccessfulFailover' 'Failure: {failureType}, failed host: {failedHost} with {countSlaves} slaves' || true"

      PostMasterFailoverProcesses:
        - "/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostMasterFailover' 'Failure type: {failureType}, new master: {successorHost}, slaves: {slaveHosts}' || true"

      PostIntermediateMasterFailoverProcesses:
        - "/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostIntermediateMasterFailover' 'Failure type: {failureType}, failed hosts: {failedHost}, slaves: {countSlaves}' || true"

      # PostGracefulTakeoverProcesses:

mariadb-operator:
  nameOverride: ""
  fullnameOverride: ""

  image:
    repository: ghcr.io/mariadb-operator/mariadb-operator
    pullPolicy: IfNotPresent
    # -- Image tag to use. By default the chart appVersion is used
    tag: ""

  # -- Controller log level
  logLevel: INFO

  # -- Cluster DNS name
  clusterName: cluster.local

  ha:
    # -- Enable high availability
    enabled: false
    # -- Number of replicas
    replicas: 3
    # -- Lease resource name to be used for leader election
    leaseId: mariadb.mmontes.io

  metrics:
    # -- Enable prometheus metrics. Prometheus must be installed in the cluster
    enabled: false
    serviceMonitor:
      # -- Enable controller ServiceMonitor
      enabled: true
      # -- Labels to be added to the controller ServiceMonitor
      additionalLabels: {}
      # release: kube-prometheus-stack
      # --  Interval to scrape metrics
      interval: 30s
      # -- Timeout if metrics can't be retrieved in given time interval
      scrapeTimeout: 25s

  webhook:
    # -- Enable webhooks. Cert-manager must be installed in the cluster
    enabled: true
    # -- Annotations for webhook configurations.
    annotations: {}
    image:
      repository: ghcr.io/mariadb-operator/mariadb-operator
      pullPolicy: IfNotPresent
      # -- Image tag to use. By default the chart appVersion is used
      tag: ""
    certificate:
      # -- Use cert-manager to issue and rotate the certificate. If set to false, a default certificate will be used.
      certManager: true
      # -- Default certificate generated when the chart is installed or upgraded.
      default:
        # -- Certificate authority expiration in days.
        caExpirationDays: 365
        # -- Certificate expiration in days.
        certExpirationDays: 365
        # -- Annotations for certificate Secret.
        annotations: {}
      # -- Path where the certificate will be mounted.
      path: /tmp/k8s-webhook-server/serving-certs
    # -- Port to be used by the webhook server
    port: 10250
    # -- Expose the webhook server in the host network
    hostNetwork: false
    serviceMonitor:
      # -- Enable webhook ServiceMonitor. Metrics must be enabled
      enabled: true
      # -- Labels to be added to the webhook ServiceMonitor
      additionalLabels: {}
      # release: kube-prometheus-stack
      # --  Interval to scrape metrics
      interval: 30s
      # -- Timeout if metrics can't be retrieved in given time interval
      scrapeTimeout: 25s
    # -- Annotations to add to webhook Pod
    podAnnotations: {}
    # -- Security context to add to webhook Pod
    podSecurityContext: {}
    # -- Security context to add to webhook container
    securityContext: {}
    # -- Resources to add to webhook container
    resources: {}
    # requests:
    #   cpu: 10m
    #   memory: 32Mi
    # -- Node selectors to add to controller Pod
    nodeSelector: {}
    # -- Tolerations to add to controller Pod
    tolerations: []
    # -- Affinity to add to controller Pod
    affinity: {}

  # -- Annotations to add to controller Pod
  podAnnotations: {}

  # -- Security context to add to controller Pod
  podSecurityContext: {}

  # -- Security context to add to controller container
  securityContext: {}

  resources: {}
  # -- Resources to add to controller container
  # requests:
  #   cpu: 10m
  #   memory: 32Mi

  # -- Node selectors to add to controller Pod
  nodeSelector: {}

  # -- Tolerations to add to controller Pod
  tolerations: []

  # -- Affinity to add to controller Pod
  affinity: {}


mysql:
  # Default values for mysql.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  image:
    registry: 
    repository: mysql
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    #tag: 8.0.33
    tag: 5.7
    xtraBackupRepository: perconalab/percona-xtrabackup

  ## MySQL Cluster parameters
  cluster:
    ## CLUSTER_ID
    clusterId: "1"
    ## CLUSTER_START_INDEX
    clusterStartIndex: "1"
    ## @param cluster.replicaSetCount
    replicaSetCount: 3
    ## MYSQL_TEMPLATE_CONFIG
    templateConfig:
    ## MYSQL_CUSTOM_CONFIG
    customConfig:
    ## MYSQL_DYNAMIC_CONFIG
    dynamicConfig:
    ## KB_EMBEDDED_WESQL
    kbWeSQLImage: "1"

  ## MySQL Authentication parameters
  auth:
    ## MYSQL_ROOT_HOST
    rootHost: "%"
    ## @param auth.createDatabase Whether to create the .Values.auth.database or not
    ##
    createDatabase: true
    ## @param auth.database Name for a custom database to create
    ## MYSQL_DATABASE
    database: "mydb"
    ## @param auth.username Name for a custom user to create
    ## MYSQL_USER
    username: "u1"
    ## MYSQL_PASSWORD
    password: "u1"
    ## @param auth.replicationUser MySQL replication user
    ##
    replicationUser: "replicator"
    ## @param auth.replicationPassword MySQL replication user password. Ignored if existing secret is provided
    ##
    replicationPassword: ""


  configTemplate:
    ## @param config template name
    ## name: mysql-3node-tpl-8.0

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  clusterVersionOverride: ""

  logConfigs:
    error: /var/lib/mysql/log/mysqld-error.log
    slow: /var/lib/mysql/log/mysqld-slowquery.log
    # general: /var/lib/mysql/log/mysqld.log

  roleProbe:
    failureThreshold: 2
    periodSeconds: 1
    timeoutSeconds: 1

  metrics:
    image:
      repository: apecloud/agamotto
      tag: 0.1.2-beta.1
      pullPolicy: IfNotPresent

    service:
      port: 9104

  dataMountPath: /var/lib/mysql


pxc-db:
  # Default values for pxc-cluster.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  finalizers:
    - delete-pxc-pods-in-order
  ## Set this if you want to delete proxysql persistent volumes on cluster deletion
  #  - delete-proxysql-pvc
  ## Set this if you want to delete database persistent volumes on cluster deletion
  #  - delete-pxc-pvc
  ## Set this if you want to delete cert manager certificates on cluster deletion
  #  - delete-ssl

  nameOverride: ""
  fullnameOverride: ""

  # PerconaXtraDBCluster annotations
  annotations: {}

  operatorImageRepository: percona/percona-xtradb-cluster-operator

  ignoreAnnotations: []
    # - iam.amazonaws.com/role
  ignoreLabels: []
    # - rack
  pause: false
  initImage: ""
  allowUnsafeConfigurations: false
  updateStrategy: SmartUpdate
  upgradeOptions:
    versionServiceEndpoint: https://check.percona.com
    apply: disabled
    schedule: "0 4 * * *"
  enableCRValidationWebhook: false
  tls: {}
    # SANs:
    #   - pxc-1.example.com
    #   - pxc-2.example.com
    #   - pxc-3.example.com
    # issuerConf:
    #   name: special-selfsigned-issuer
    #   kind: ClusterIssuer
    #   group: cert-manager.io

  pxc:
    size: 3
    image:
      repository: percona/percona-xtradb-cluster

    # imagePullPolicy: Always
    autoRecovery: true
    # expose:
    #   enabled: true
    #   type: LoadBalancer
    #   trafficPolicy: Local
    #   loadBalancerSourceRanges:
    #   - 10.0.0.0/8
    #   annotations:
    #     networking.gke.io/load-balancer-type: "Internal"
    # replicationChannels:
    # - name: pxc1_to_pxc2
    #   isSource: true
    # - name: pxc2_to_pxc1
    #   isSource: false
    #   configuration:
    #     sourceRetryCount: 3
    #     sourceConnectRetry: 60
    #     ssl: false
    #     sslSkipVerify: true
    #     ca: '/etc/mysql/ssl/ca.crt'
    #   sourcesList:
    #   - host: 10.95.251.101
    #     port: 3306
    #     weight: 100
    # schedulerName: mycustom-scheduler
    imagePullSecrets: []
    # - name: private-registry-credentials
    annotations: {}
    #  iam.amazonaws.com/role: role-arn
    labels: {}
    #  rack: rack-22
    # priorityClassName: high-priority
    readinessDelaySec: 15
    livenessDelaySec: 300
    ## Uncomment to pass in a mysql config file
    # configuration: |
    #   [mysqld]
    #   wsrep_debug=ON
    #   wsrep_provider_options="gcache.size=1G; gcache.recover=yes"
    # envVarsSecret: my-env-var-secrets
    resources:
      requests:
        memory: 1G
        cpu: 600m
      limits: {}
        # memory: 1G
        # cpu: 600m
    # runtimeClassName: image-rc
    sidecars: []
    sidecarVolumes: []
    sidecarPVCs: []
    sidecarResources:
      requests: {}
      limits: {}
    nodeSelector: {}
    #  disktype: ssd
    affinity:
      antiAffinityTopologyKey: "kubernetes.io/hostname"
      # advanced:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #       - matchExpressions:
      #         - key: kubernetes.io/e2e-az-name
      #           operator: In
      #           values:
      #           - e2e-az1
      #           - e2e-az2
    tolerations: []
      # - key: "node.alpha.kubernetes.io/unreachable"
      #   operator: "Exists"
      #   effect: "NoExecute"
      #   tolerationSeconds: 6000
    gracePeriod: 600
    podDisruptionBudget:
      # only one of maxUnavailable or minAvaliable can be set
      maxUnavailable: 1
      # minAvailable: 0
    persistence:
      enabled: true
      ## percona data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"
      accessMode: ReadWriteOnce
      size: 8Gi

    # If you set this to true the cluster will be created without TLS
    disableTLS: false

    # disable Helm creating TLS certificates if you want to let the operator
    # request certificates from cert-manager
    certManager: false

    # If this is set will not create secrets from values and will instead try to use
    # a pre-existing secret of the same name.
    # clusterSecretName: cluster1-secrets
    readinessProbes:
      initialDelaySeconds: 15
      timeoutSeconds: 15
      periodSeconds: 30
      successThreshold: 1
      failureThreshold: 5
    livenessProbes:
      initialDelaySeconds: 300
      timeoutSeconds: 5
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 3
    # A custom Kubernetes Security Context for a Container to be used instead of the default one
    # containerSecurityContext:
    #   privileged: false
    # A custom Kubernetes Security Context for a Pod to be used instead of the default one
    # podSecurityContext:
    #   fsGroup: 1001
    #   supplementalGroups:
    #   - 1001
    # serviceAccountName: percona-xtradb-cluster-operator-workload

  haproxy:
    enabled: true
    size: 3
    image: ""
    # imagePullPolicy: Always
    imagePullSecrets: []
    # - name: private-registry-credentials
  #  configuration: |
  #
  #  the actual default configuration file can be found here https://github.com/percona/percona-docker/blob/main/haproxy/dockerdir/etc/haproxy/haproxy-global.cfg
  #
  #    global
  #      maxconn 2048
  #      external-check
  #      insecure-fork-wanted
  #      stats socket /etc/haproxy/pxc/haproxy.sock mode 600 expose-fd listeners level admin
  #
  #    defaults
  #      default-server init-addr last,libc,none
  #      log global
  #      mode tcp
  #      retries 10
  #      timeout client 28800s
  #      timeout connect 100500
  #      timeout server 28800s
  #
  #    frontend galera-in
  #      bind *:3309 accept-proxy
  #      bind *:3306
  #      mode tcp
  #      option clitcpka
  #      default_backend galera-nodes
  #
  #    frontend galera-admin-in
  #      bind *:33062
  #      mode tcp
  #      option clitcpka
  #      default_backend galera-admin-nodes
  #
  #    frontend galera-replica-in
  #      bind *:3307
  #      mode tcp
  #      option clitcpka
  #      default_backend galera-replica-nodes
  #
  #    frontend galera-mysqlx-in
  #      bind *:33060
  #      mode tcp
  #      option clitcpka
  #      default_backend galera-mysqlx-nodes
  #
  #    frontend stats
  #      bind *:8404
  #      mode http
  #      option http-use-htx
  #      http-request use-service prometheus-exporter if { path /metrics }
    annotations: {}
    #  iam.amazonaws.com/role: role-arn
    labels: {}
    #  rack: rack-22
    # serviceType: ClusterIP
    # externalTrafficPolicy: Cluster
    # runtimeClassName: image-rc
    # loadBalancerSourceRanges:
    #   - 10.0.0.0/8
    # loadBalancerIP: 127.0.0.1
    # serviceAnnotations:
    #   service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
    # serviceLabels:
    #   rack: rack-23
    replicasServiceEnabled: true
    # replicasLoadBalancerSourceRanges:
    #   - 10.0.0.0/8
    # replicasLoadBalancerIP: 127.0.0.1
    # replicasServiceType: ClusterIP
    # replicasExternalTrafficPolicy: Cluster
    # replicasServiceAnnotations:
    #   service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
    # replicasServiceLabels:
    #   rack: rack-23
    # priorityClassName: high-priority
    # schedulerName: mycustom-scheduler
    readinessDelaySec: 15
    livenessDelaySec: 300
    # envVarsSecret: my-env-var-secrets
    resources:
      requests:
        memory: 1G
        cpu: 600m
      limits: {}
        # memory: 1G
        # cpu: 600m
    sidecars: []
    sidecarVolumes: []
    sidecarPVCs: []
    sidecarResources:
      requests: {}
      limits: {}
    nodeSelector: {}
    #  disktype: ssd
    # serviceAccountName: percona-xtradb-cluster-operator-workload
    affinity:
      antiAffinityTopologyKey: "kubernetes.io/hostname"
      # advanced:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #       - matchExpressions:
      #         - key: kubernetes.io/e2e-az-name
      #           operator: In
      #           values:
      #           - e2e-az1
      #           - e2e-az2
    tolerations: []
      # - key: "node.alpha.kubernetes.io/unreachable"
      #   operator: "Exists"
      #   effect: "NoExecute"
      #   tolerationSeconds: 6000
    gracePeriod: 30
    # only one of `maxUnavailable` or `minAvailable` can be set.
    podDisruptionBudget:
      maxUnavailable: 1
      # minAvailable: 0
    readinessProbes:
      initialDelaySeconds: 15
      timeoutSeconds: 1
      periodSeconds: 5
      successThreshold: 1
      failureThreshold: 3
    livenessProbes:
      initialDelaySeconds: 60
      timeoutSeconds: 5
      periodSeconds: 30
      successThreshold: 1
      failureThreshold: 4
    # A custom Kubernetes Security Context for a Container to be used instead of the default one
    # containerSecurityContext:
    #   privileged: false
    # A custom Kubernetes Security Context for a Pod to be used instead of the default one
    # podSecurityContext:
    #   fsGroup: 1001
    #   supplementalGroups:
    #   - 1001

  proxysql:
    enabled: false
    size: 3
    image: ""
    # imagePullPolicy: Always
    imagePullSecrets: []
  #  configuration: |
  #    datadir="/var/lib/proxysql"
  #
  #    admin_variables =
  #    {
  #      admin_credentials="proxyadmin:admin_password"
  #      mysql_ifaces="0.0.0.0:6032"
  #      refresh_interval=2000
  #
  #      cluster_username="proxyadmin"
  #      cluster_password="admin_password"
  #      checksum_admin_variables=false
  #      checksum_ldap_variables=false
  #      checksum_mysql_variables=false
  #      cluster_check_interval_ms=200
  #      cluster_check_status_frequency=100
  #      cluster_mysql_query_rules_save_to_disk=true
  #      cluster_mysql_servers_save_to_disk=true
  #      cluster_mysql_users_save_to_disk=true
  #      cluster_proxysql_servers_save_to_disk=true
  #      cluster_mysql_query_rules_diffs_before_sync=1
  #      cluster_mysql_servers_diffs_before_sync=1
  #      cluster_mysql_users_diffs_before_sync=1
  #      cluster_proxysql_servers_diffs_before_sync=1
  #    }
  #
  #    mysql_variables=
  #    {
  #      monitor_password="monitor"
  #      monitor_galera_healthcheck_interval=1000
  #      threads=2
  #      max_connections=2048
  #      default_query_delay=0
  #      default_query_timeout=10000
  #      poll_timeout=2000
  #      interfaces="0.0.0.0:3306"
  #      default_schema="information_schema"
  #      stacksize=1048576
  #      connect_timeout_server=10000
  #      monitor_history=60000
  #      monitor_connect_interval=20000
  #      monitor_ping_interval=10000
  #      ping_timeout_server=200
  #      commands_stats=true
  #      sessions_sort=true
  #      have_ssl=true
  #      ssl_p2s_ca="/etc/proxysql/ssl-internal/ca.crt"
  #      ssl_p2s_cert="/etc/proxysql/ssl-internal/tls.crt"
  #      ssl_p2s_key="/etc/proxysql/ssl-internal/tls.key"
  #      ssl_p2s_cipher="ECDHE-RSA-AES128-GCM-SHA256"
  #    }
    # - name: private-registry-credentials
    annotations: {}
    #  iam.amazonaws.com/role: role-arn
    labels: {}
    #  rack: rack-22
    # serviceType: ClusterIP
    # externalTrafficPolicy: Cluster
    # runtimeClassName: image-rc
    # loadBalancerSourceRanges:
    #   - 10.0.0.0/8
    # loadBalancerIP: 127.0.0.1
    # serviceAnnotations:
    #   service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
    # serviceLabels:
    #   rack: rack-23
    # priorityClassName: high-priority
    # schedulerName: mycustom-scheduler
    readinessDelaySec: 15
    livenessDelaySec: 300
    # envVarsSecret: my-env-var-secrets
    resources:
      requests:
        memory: 1G
        cpu: 600m
      limits: {}
        # memory: 1G
        # cpu: 600m
    sidecars: []
    sidecarVolumes: []
    sidecarPVCs: []
    sidecarResources:
      requests: {}
      limits: {}
    nodeSelector: {}
    #  disktype: ssd
    # serviceAccountName: percona-xtradb-cluster-operator-workload
    affinity:
      antiAffinityTopologyKey: "kubernetes.io/hostname"
      # advanced:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #       - matchExpressions:
      #         - key: kubernetes.io/e2e-az-name
      #           operator: In
      #           values:
      #           - e2e-az1
      #           - e2e-az2
    tolerations: []
      # - key: "node.alpha.kubernetes.io/unreachable"
      #   operator: "Exists"
      #   effect: "NoExecute"
      #   tolerationSeconds: 6000
    gracePeriod: 30
    # only one of `maxUnavailable` or `minAvailable` can be set.
    podDisruptionBudget:
      maxUnavailable: 1
      # minAvailable: 0
    persistence:
      enabled: true
      ## Percona data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"
      accessMode: ReadWriteOnce
      size: 8Gi
    # A custom Kubernetes Security Context for a Container to be used instead of the default one
    # containerSecurityContext:
    #   privileged: false
    # A custom Kubernetes Security Context for a Pod to be used instead of the default one
    # podSecurityContext:
    #   fsGroup: 1001
    #   supplementalGroups:
    #   - 1001

  logcollector:
    enabled: true
    image: ""
    # imagePullPolicy: Always
    imagePullSecrets: []
    # configuration: |
    #   [OUTPUT]
    #         Name  es
    #         Match *
    #         Host  192.168.2.3
    #         Port  9200
    #         Index my_index
    #         Type  my_type
    resources:
      requests:
        memory: 100M
        cpu: 200m
      limits: {}

  pmm:
    enabled: false
    image:
      repository: percona/pmm-client

    # imagePullPolicy: Always
    imagePullSecrets: []
    serverHost: monitoring-service
    serverUser: admin
    # pxcParams: "--disable-tablestats-limit=2000"
    # proxysqlParams: "--custom-labels=CUSTOM-LABELS"
    resources:
      requests:
        memory: 150M
        cpu: 300m
      limits: {}

  backup:
    enabled: true
    # allowParallel: true
    image:
      repository: percona/percona-xtradb-cluster-operator

    # backoffLimit: 6
    # serviceAccountName: percona-xtradb-cluster-operator
    # imagePullPolicy: Always
    imagePullSecrets: []
    # - name: private-registry-credentials
    pitr:
      enabled: false
      storageName: s3-us-west-binlogs
      timeBetweenUploads: 60
      resources:
        requests: {}
        limits: {}
    storages: {}
      # fs-pvc:
      #   type: filesystem
      #   volume:
      #     persistentVolumeClaim:
      #       storageClassName: standard
      #       accessModes: ["ReadWriteOnce"]
      #       resources:
      #         requests:
      #           storage: 6Gi
      # s3-us-west:
      #   type: s3
      #   verifyTLS: true
      #   nodeSelector:
      #     storage: tape
      #     backupWorker: 'True'
      #   resources:
      #     requests:
      #       memory: 1G
      #       cpu: 600m
      #   affinity:
      #     nodeAffinity:
      #       requiredDuringSchedulingIgnoredDuringExecution:
      #         nodeSelectorTerms:
      #         - matchExpressions:
      #           - key: backupWorker
      #             operator: In
      #             values:
      #             - 'True'
      #   tolerations:
      #     - key: "backupWorker"
      #       operator: "Equal"
      #       value: "True"
      #       effect: "NoSchedule"
      #   annotations:
      #     testName: scheduled-backup
      #   labels:
      #     backupWorker: 'True'
      #   schedulerName: 'default-scheduler'
      #   priorityClassName: 'high-priority'
      #   containerSecurityContext:
      #     privileged: true
      #   podSecurityContext:
      #     fsGroup: 1001
      #     supplementalGroups: [1001, 1002, 1003]
      #   s3:
      #     bucket: S3-BACKUP-BUCKET-NAME-HERE
      #     # Use credentialsSecret OR credentialsAccessKey/credentialsSecretKey
      #     credentialsSecret: my-cluster-name-backup-s3
      #     #credentialsAccessKey: REPLACE-WITH-AWS-ACCESS-KEY
      #     #credentialsSecretKey: REPLACE-WITH-AWS-SECRET-KEY
      #     region: us-west-2
      #     endpointUrl: https://sfo2.digitaloceanspaces.com
      # s3-us-west-binlogs:
      #   type: s3
      #   s3:
      #     bucket: S3-BACKUP-BUCKET-NAME-HERE/DIRECTORY
      #     credentialsSecret: my-cluster-name-backup-s3
      #     region: us-west-2
      #     endpointUrl: https://sfo2.digitaloceanspaces.com
      # azure-blob:
      #   type: azure
      #   azure:
      #     credentialsSecret: azure-secret
      #     container: test
      #     endpointUrl: https://accountName.blob.core.windows.net
      #     storageClass: Hot

    schedule: []
      # - name: "daily-backup"
      #   schedule: "0 0 * * *"
      #   keep: 5
      #   storageName: fs-pvc
      # - name: "sat-night-backup"
      #   schedule: "0 0 * * 6"
      #   keep: 3
      #   storageName: s3-us-west

  secrets:
    ## You should be overriding these with your own or specify name for clusterSecretName.
    # passwords:
    #   root: insecure-root-password
    #   xtrabackup: insecure-xtrabackup-password
    #   monitor: insecure-monitor-password
    #   clustercheck: insecure-clustercheck-password
    #   proxyadmin: insecure-proxyadmin-password
    #   pmmserver: insecure-pmmserver-password
    #   # If pmmserverkey is set in that case pmmserver pass will not be included
    #   # pmmserverkey: set-pmmserver-api-key
    #   operator: insecure-operator-password
    #   replication: insecure-replication-password
    ## If you are using `cert-manager` you can skip this next section.
    tls: {}
      # This should be the name of a secret that contains certificates.
      # it should have the following keys: `ca.crt`, `tls.crt`, `tls.key`
      # If not set the Helm chart will attempt to create certificates
      # for you [not recommended for prod]:
      # cluster:

      # This should be the name of a secret that contains certificates.
      # it should have the following keys: `ca.crt`, `tls.crt`, `tls.key`
      # If not set the Helm chart will attempt to create certificates
      # for you [not recommended for prod]:
      # internal:
    # logCollector: cluster1-log-collector-secrets
    # vault: keyring-secret-vault
